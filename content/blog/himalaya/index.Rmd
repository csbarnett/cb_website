---
title: "Himalayan climbing expeditions"
author: "Chris Barnett"
date: "`r Sys.Date()`"
output: html_document
---
<center>
![](https://www.forbesindia.com/media/images/2022/Oct/img_195311_himalaya.jpg)
<center/>

I came across [this article](https://www.nytimes.com/2018/01/26/obituaries/elizabeth-hawley-who-chronicled-everest-treks-dies-at-94.html) about the passing of journalist Elizabeth Hawley who chronicled Himalayan mountaineering expeditions for more than 50 years and was one of the founders of the Himalayan Database. The [Himalayan Database](https://www.himalayandatabase.com) is a compilation of mountaineering climbing records for all expeditions in the Himalayas of Nepal. The database has been supplemented by information from books, alpine journals, and correspondence with Himalayan climbers. Elizabeth Hawley took the accuracy of the database seriously. She would often interview mountaineers (more than 15,000 interviews) before and after summit attempts and ask about specific details to gauge authenticity. Elizabeth Hawley died in January 2018 in a small hospital in Nepal. She was 94.

The data includes expeditions from 1905 to (currently) Spring 2022. Expeditions on peaks that span the border of Nepal and China, such as Everest, Cho Oyu, Makalu, and Kangchenjunga, and some smaller border peaks are also included. Each expedition record contains detailed information, including dates, routes, camps, use of supplemental oxygen, successes, deaths, and accidents. Each expedition record includes biographical information for all members listed on the permit and for hired members (e.g., Sherpas) for which there are significant events such as a summit success, death, accident, or rescue. We will explore the dataset and see if we can develop a simple model to predict a climber's success in summiting.

```{r setup, include=FALSE}
library(tidyverse)
library(tidymodels)

himal <- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-22/members.csv")

glimpse(himal)
```

## EDA
What are the Hardest peaks to climb in the Himalayas? To find out, we will sort each peak by the **success rate** (the number of ascents divided by the number of attempts). We will also restrict the data to peaks with at least 100 attempts to summit.
```{r pressure, echo=FALSE}
peaks <- himal %>%
  filter(!is.na(peak_name)) %>%
  #mutate(peak_name = fct_lump(peak_name, prop = 0.01)) %>%
  count(peak_name, success) %>%
  group_by(peak_name) %>%
  mutate(attempts = sum(n)) %>%
  mutate(success_rate = round((n/attempts)*100, 2)) %>%
  filter(success == TRUE) %>%
  arrange(success_rate) %>%
  select(-success) %>%
  rename("ascents" = n) %>%
  filter(attempts > 100)
peaks


  himal %>%
  filter(!is.na(peak_name)) %>%
  count(oxygen_used, success) %>%
  group_by(oxygen_used) %>%
  mutate(attempts = sum(n)) %>%
  mutate(success_rate = round((n/attempts)*100, 2)) %>%
  filter(success == TRUE)
  
  himal %>%
  filter(!is.na(peak_name)) %>%
  count(solo, success) %>%
  group_by(solo) %>%
  mutate(attempts = sum(n)) %>%
  mutate(success_rate = round((n/attempts)*100, 2)) %>%
  filter(success == TRUE)
```
![](https://www.caingram.info/Nepalpeaks/Pix/Peak_29.jpg)

Peak 29 or "Ngadi Chuli"

```{r}
# Filtering himal_df by only peaks with 100 or more attempts
himal_100 <- himal %>%
  count(peak_id, success) %>%
  group_by(peak_id) %>%
  mutate(attempts = sum(n)) %>%
  filter(success == TRUE) %>%
  filter(attempts > 100)

himal_100 %>% distinct(peak_id)

```

The filter join **semi_join()** of himal using himal_100 gives us only himal observations with peaks that are present in himal_100.

```{r}
himal_100_df <- himal %>%
  semi_join(himal_100, by = "peak_id")

himal_100_df %>% distinct(peak_id)
```
Taking a quick look at the himal_100_df data with the *skimr package*
```{r}
skimr::skim(himal_100_df)
```
Now, we will look at the possible values for each character value (minus member ID and expedition ID). This will give us an idea of how many levels we will deal with if we transform these variables into factors.
```{r}
# Selecting character variables and looping levels function over each variable
himal_100_df %>% select_if(.predicate = is.character) %>%
  select(-member_id, -expedition_id) %>%
  map(~levels(factor(.x)))
```

## Creating modeling dataframe
Here we will select the features (variables) we will include in our model. We will used variables *peak_id*, *year*, *season*, *sex*, *age*, *citizenship*, *hired*, *solo*, and *oxygen_used* to try and predict *success*.  
```{r}
# Removing observation where season is not known and sex and citizenship is NA
himal_df <- himal_100_df %>%
  filter(season != "Unknown", 
         !is.na(sex), 
         !is.na(citizenship)) %>%
  select(peak_id, year, season, sex, age, citizenship, 
         hired, success, solo, oxygen_used) %>%
  mutate_if(is.character, factor) %>%
  mutate_if(is.logical, as.integer) %>%
  mutate(success = factor(success, levels = c(0, 1), labels = c("fail", "success")))
```


Let's pause here to see how balanced our data is.
```{r}
# Checking outcome balance
table(himal_df$success)
```
Our outcome is not perfectly balanced, but it should be fine!


## Splitting data
```{r}
# Creating training and test splits. Stratifying by outcome: success
set.seed(111)
himal_df_split <- initial_split(himal_df, strata = success)
himal_train <- training(himal_df_split)
himal_test <- testing(himal_df_split)
```

## Creating Ten-fold Cross-Validation Folds
```{r}
# Creating 10 fold cross-validation folds. Stratifying by outcome: success
set.seed(222)
himal_folds <- vfold_cv(himal_train, strata = success)
himal_folds
```

## Preprocessing recipe
- First, we will define the model within the recipe. 
- Next, we will create a high cardinality mini-model for peak_id where the mini-model step will convert the nominal (i.e., factor) predictor (peak_id) into a single set of scores derived from a generalized linear model.
- From our **skim()** of the dataframe, we saw over two thousand rows with a missing age variable. Instead of removing those rows, we will impute the median age to be able to get the rest of the data in those rows.
- Like peak_id, the Variable *citizenship* also has high cardinality. However, *citizenship's* cardinality is over four times that of peak_id, full of many small cell counts and multi-country combinations. To address this, we will create a new category (other) within *citizenship* to capture all instances representing less than 5 percent of the data.
- Lastly, we will use at least one prediction model that does not handle factor variables. We must create dummy variables from each of our dataframe's factor variables, except for our outcome variable *success*.

```{r}
library(embed)
himal_recip <- recipe(success ~ ., data = himal_train) %>%
  embed::step_lencode_glm(peak_id, outcome = vars(success)) %>%
  step_impute_median(age) %>%
  step_other(citizenship) %>%
  step_dummy(all_nominal(), -success)

himal_recip
```
We now have our data recipe. We must run it through the **prep()** and **bake()** functions to see it applied to the training data.
```{r}
prep(himal_recip) %>%
  bake(new_data = NULL)
```


Let's look at the tidied result of the first step of our recipe, where we convert the predictor *peak_id* into a single set of scores derived from a generalized linear model. 
```{r}
prep(himal_recip) %>%
  tidy(number = 1) %>%
  arrange(value)
```
Here we can see that each peak has been converted to the quantified association with the outcome variable *success* and that the same peaks we identified as having the lowest success rate in our EDA are also negatively associated with success in our new peak_id variable.

However, this is a living database that is still being updated. How would ascents of new peaks be predicted by this model? You may notice a new level within peak_id *..new*. This is a placeholder for any encounter of a new peak that includes a value that roughly equals the median value of all the peaks. Therefore, the *embed* package and the **step_lencode_glm()** function have built-in imputation!

```{r}
prep(himal_recip) %>%
  tidy(number = 1) %>%
  filter(level == "..new")
```


## Model Specifications
Logistic regression and random forest are not the most complex prediction algorithms. However, I like using these two models early in my prediction journey for many reasons. First, they are both quick to run, and the results are far more interpretable than many other algorithms. Second, I often don't bother to tune any hyperparameters because 1) they are no hyperparameters to tune for logistic regression,  or 2) random forests often perform well without extensive hyperparameter tuning. 

However, for this demonstration, we will tune the random forest model to optimize model fit.
```{r}
# Creating a logistic regression model specification
log_spec <- logistic_reg() %>%
  set_engine("glm")

# Creating a random forest model specification with tuning parameters
rf_spec <- rand_forest(mtry = tune(),
                       min_n = tune(), 
                       trees = 1000) %>%
  set_engine("ranger") %>%
  set_mode("classification")
```


## Creating Workflows
[Workflows](https://workflows.tidymodels.org) are the tidymodel's method of compartmentalizing (or bundling) the pre-processing, modeling, (and even post-processing) steps of your modeling process.   

### Logistic Regression Model
We will name our logistic regression model workflow "*log_wf*," which consists of our recipe and model specification.
```{r}
# Building logistic regression workflow
log_wf <- workflow() %>%
  add_recipe(himal_recip) %>%
  add_model(log_spec)
```

Here we will turn our logistic regression model loose on the cross-validation folds from the training data.
```{r}
# Fitting the logistic regression model to the cross-validation folds from the
# training data.
log_wf_train <- log_wf %>%
  fit_resamples(
    resamples = himal_folds,
    metrics = metric_set(roc_auc, accuracy, sensitivity, specificity), 
    control = control_resamples(save_pred = TRUE)
    )
```

### Random Forest Model 
Here, we are naming our workflow "*rf_wf*," which consists of our recipe and model specification.
```{r}
# Building random forest workflow
rf_wf <- workflow() %>%
  add_recipe(himal_recip) %>%
  add_model(rf_spec) 
```


Now we will tune the *mtry* and *min_n* hyperparameters in our random forest model using 13 parameter sets.
```{r}
set.seed(333)
doParallel::registerDoParallel()
rf_train_rs <- tune_grid(object = rf_wf,
                         resamples = himal_folds, 
                         grid = 13, 
                         metrics = metric_set(roc_auc, accuracy, sensitivity, specificity), 
                         control = control_resamples(save_pred = TRUE)
    )
```
![](https://media.gadventures.com/media-server/image_library/variants/itinerary_mobi_Nepal-Himalaya-Mountains-Annapurna-Pokhara-Prayer-Flags-IS-027332084-Lg-RGB.jpg)
Tashiling Tibetan Refugee Camp (Photo Credit: [G Adventures](https://www.gadventures.com/trips/nepal-himalaya-highlights/ANENG/))

## Model Evaluation
### Logistic Regression
```{r}
collect_metrics(log_wf_train)
```
Our logistic regression model accurately identified about 77% percent of summit attempts correctly, with an Area Under the Curve (AUC) of over 80% percent. Not horrible, but let's see if our random forest model can outperform that.

### Random Forest
For our random forest model, we will plot the values for accuracy, roc_auc, sensitivity, and specificity. The result of the tuning object works seamlessly with the **autoplot()** function. Next, we will identify the tuning parameters with the highest accuracy and roc_auc.
```{r}
autoplot(rf_train_rs)

collect_metrics(rf_train_rs) %>%
  filter(.metric == "accuracy") %>%
  arrange(desc(mean))

collect_metrics(rf_train_rs) %>%
  filter(.metric == "roc_auc") %>%
  arrange(desc(mean))

```

The output above shows that a *mtry* value of 7 and a min_n value of 16 produce the highest accuracy and roc_auc on the cross-validation training folds. We can easily select the model with the best hyperparameters with the **select_best()** function.
```{r}
# Defining the best performing random forest model (by mtry) based on accuracy
best_rf_model <- select_best(rf_train_rs, metric = "accuracy", mtry)

best_rf_model

```

## Finalize Model
### Finalized Workflow
```{r}
# Specifying final random forest workflow with best performing tunning model
final_rf_wf <- finalize_workflow(rf_wf, parameters = best_rf_model)
final_rf_wf
```
### Finalized Model Fit
```{r}
# Fitting final random forest workflow to training data
set.seed(444)
doParallel::registerDoParallel()
final_rf_fit <- fit(final_rf_wf, himal_train)
final_rf_fit

# saving the model
# saveRDS(final_rf_fit, file = "your_filepath_here/final_rf_fit.rda")
```
We now have a trained workflow consisting of the preprocessing steps and a tuned model (tuned on cv-folds from the training data). We can save this object and use it to predict on new data in the future.

### Evaluating Model on Test Data
Luckily, we don't have to wait for new climbing ascents. We can evaluate our models on the test data we created in our initial data split. 

First, we will evaluate our logistic regression model.
```{r}
# Fitting the logistic regression model to the testing data
himal_log_final_fit <- log_wf %>%
  last_fit(split = himal_df_split)

# Collecting test accuracy and roc_auc for logistic regression model
collect_metrics(himal_log_final_fit)

# Creating a confusion matrix for model performance on test data
collect_predictions(himal_log_final_fit) %>%
  conf_mat(success, .pred_class)
```
Our logistic regression metrics on the test data are almost exactly equal to the training metrics (the test accuracy and roc_auc are actually a little better than the training metrics). Therefore, we certainly didn't overfit our training data with the logistic regression model.


Now we will evaluate our random forest model on the test data.
```{r}
# Predicting success on new (test) data

# doParallel::registerDoParallel()
# test_rs <- predict(final_rf_fit, new_data = himal_test) %>%
#   bind_cols(himal_test) %>%
#   select(peak_id:hired, solo, oxygen_used, success, .pred_class)
# 
# test_rs %>%
#   mutate(.pred_class = as.numeric(.pred_class)) %>%
#   roc_curve(truth = success, .pred_class) %>%
#   autoplot()

# Enabling parallel-processing
doParallel::registerDoParallel()

set.seed(555)
# Predicting outcomes on test data with random forest model
himal_rf_final_fit <- final_rf_wf %>%
  last_fit(split = himal_df_split)

# Collecting test accuracy and roc_auc for random forest model
collect_metrics(himal_rf_final_fit)

# Creating a confusion matrix for model performance on test data
collect_predictions(himal_rf_final_fit) %>%
  conf_mat(success, .pred_class)

# Ploting ROC AUC curve of random forest model performance
collect_predictions(himal_rf_final_fit) %>%
  roc_curve(truth = success, .pred_success, event_level = "second") %>%
  autoplot()
```

With our random forest model and our hyper-tuning, we increased our accuracy to over 0.81 and our AUC to nearly **0.89** on the test data. Not a bad a classifier for a fairly simple model. 

## Variable Importance
Unfortunately, we didn't set up our random forest *ranger* engine to be able to look at variable importance. Fortunately, we can lean on our logistic regression model to quantify the contribution of each predictor.


First, we can pull() the S3 object (.workflow) from the final logistic regression fit. Next, we can pluck the trained workflow and tidy the results. Here, we will exponentiate the coefficient estimates to get odds ratios.
```{r}
# Plucking the trained workflow from the final logistic regression model fit
himal_log_final_fit %>%
  pull(.workflow) %>%
  pluck(1) %>%
  tidy(exponentiate = TRUE) %>% 
  arrange(estimate) %>%
  knitr::kable(digits = 3)
```

We see that specific predictors like "*oxygen_used*" and "*solo*" seem to be strongly correlated with success, while other variables like "*season_Summer*" and "*season_Spring*" appear to be negatively correlated with a climber's probability of reaching the summit.

We can easily keep the estimated values on the log scale, which is often better for visualizing the data.
```{r}
himal_log_final_fit %>%
  pull(.workflow) %>%
  pluck(1) %>%
  tidy(exponentiate = FALSE) %>%
  arrange(estimate) %>%
  knitr::kable(digits = 3)
```


Let's plot this to make it easier to see the contribution of each predictor in our logistic regression model.
```{r}
himal_log_final_fit %>%
  pull(.workflow) %>%
  pluck(1) %>%
  tidy(exponentiate = FALSE) %>%
  filter(term != "(Intercept)") %>%
  ggplot(aes(estimate, fct_reorder(term, estimate))) +
  geom_vline(xintercept = 0, color = "gray50", lty = 2, size = 1.2) +
  geom_errorbar(aes(xmin = estimate - std.error, 
                    xmax = estimate + std.error), 
                width = 0.2, alpha = 0.7) +
  geom_point()
```
Interestingly, nearly all of our predictors (except for a few citizenship features) have a statistically significant relationship with the expedition's success. It is not surprising that the use of oxygen seems to significantly increase the likelihood of success. However, less intuitive is the relationship of solo climbing to our outcome. Perhaps soloists are among the most experienced and skilled climbers, and the routes and peaks they choose to solo have a lower difficulty and risk profile since they are climbing without the support of a partner or a team.

There is so much more to explore with this data set. Maybe I will revisit it in the future to answer a different question. Thanks for reading to the end!

![](https://img.traveltriangle.com/blog/wp-content/uploads/2020/01/cover-Mountaineering-in-Himalayas_17th-jan.jpg)
